{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.380091\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.258454 analytic: -0.258454, relative error: 4.003729e-08\n",
      "numerical: 3.096928 analytic: 3.096928, relative error: 3.841973e-08\n",
      "numerical: 0.723220 analytic: 0.723219, relative error: 1.201218e-07\n",
      "numerical: 1.141806 analytic: 1.141806, relative error: 5.149285e-08\n",
      "numerical: -1.068122 analytic: -1.068122, relative error: 3.048399e-08\n",
      "numerical: -1.597298 analytic: -1.597298, relative error: 9.495832e-09\n",
      "numerical: 3.149509 analytic: 3.149509, relative error: 3.442870e-08\n",
      "numerical: 1.032228 analytic: 1.032228, relative error: 2.448341e-08\n",
      "numerical: 0.855192 analytic: 0.855192, relative error: 5.514331e-08\n",
      "numerical: 0.707640 analytic: 0.707640, relative error: 5.312921e-09\n",
      "Add Regularization:\n",
      "numerical: 2.109576 analytic: 2.109576, relative error: 4.947742e-09\n",
      "numerical: -0.851304 analytic: -0.851304, relative error: 3.496297e-09\n",
      "numerical: -0.871350 analytic: -0.871350, relative error: 1.557141e-10\n",
      "numerical: -2.880013 analytic: -2.880013, relative error: 9.109316e-09\n",
      "numerical: 0.882088 analytic: 0.882088, relative error: 2.111925e-08\n",
      "numerical: 0.911604 analytic: 0.911604, relative error: 4.006978e-08\n",
      "numerical: 0.307241 analytic: 0.307241, relative error: 1.551154e-07\n",
      "numerical: -0.514395 analytic: -0.514395, relative error: 1.388319e-08\n",
      "numerical: -2.070238 analytic: -2.070238, relative error: 3.093491e-09\n",
      "numerical: 0.026161 analytic: 0.026160, relative error: 2.559020e-06\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "print(\"Add Regularization:\")\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.380091e+00 computed in 0.111167s\n",
      "(3073, 10)\n",
      "vectorized loss: 2.380091e+00 computed in 0.008012s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 770.256707\n",
      "iteration 100 / 1500: loss 282.687355\n",
      "iteration 200 / 1500: loss 104.764371\n",
      "iteration 300 / 1500: loss 39.636600\n",
      "iteration 400 / 1500: loss 15.835766\n",
      "iteration 500 / 1500: loss 7.103691\n",
      "iteration 600 / 1500: loss 3.932914\n",
      "iteration 700 / 1500: loss 2.768650\n",
      "iteration 800 / 1500: loss 2.352435\n",
      "iteration 900 / 1500: loss 2.185736\n",
      "iteration 1000 / 1500: loss 2.133238\n",
      "iteration 1100 / 1500: loss 2.106870\n",
      "iteration 1200 / 1500: loss 2.083034\n",
      "iteration 1300 / 1500: loss 2.144302\n",
      "iteration 1400 / 1500: loss 2.092354\n",
      "iteration 0 / 1500: loss 863.358966\n",
      "iteration 100 / 1500: loss 283.556049\n",
      "iteration 200 / 1500: loss 93.973688\n",
      "iteration 300 / 1500: loss 32.239326\n",
      "iteration 400 / 1500: loss 11.923746\n",
      "iteration 500 / 1500: loss 5.319950\n",
      "iteration 600 / 1500: loss 3.162611\n",
      "iteration 700 / 1500: loss 2.429707\n",
      "iteration 800 / 1500: loss 2.254342\n",
      "iteration 900 / 1500: loss 2.097663\n",
      "iteration 1000 / 1500: loss 2.155000\n",
      "iteration 1100 / 1500: loss 2.076922\n",
      "iteration 1200 / 1500: loss 2.142977\n",
      "iteration 1300 / 1500: loss 2.090791\n",
      "iteration 1400 / 1500: loss 2.102657\n",
      "iteration 0 / 1500: loss 944.819487\n",
      "iteration 100 / 1500: loss 277.616356\n",
      "iteration 200 / 1500: loss 82.722593\n",
      "iteration 300 / 1500: loss 25.781408\n",
      "iteration 400 / 1500: loss 9.057865\n",
      "iteration 500 / 1500: loss 4.113399\n",
      "iteration 600 / 1500: loss 2.705819\n",
      "iteration 700 / 1500: loss 2.198569\n",
      "iteration 800 / 1500: loss 2.141410\n",
      "iteration 900 / 1500: loss 2.124436\n",
      "iteration 1000 / 1500: loss 2.058220\n",
      "iteration 1100 / 1500: loss 2.053000\n",
      "iteration 1200 / 1500: loss 2.127108\n",
      "iteration 1300 / 1500: loss 2.128005\n",
      "iteration 1400 / 1500: loss 2.056482\n",
      "iteration 0 / 1500: loss 1030.341668\n",
      "iteration 100 / 1500: loss 271.125651\n",
      "iteration 200 / 1500: loss 72.396067\n",
      "iteration 300 / 1500: loss 20.569971\n",
      "iteration 400 / 1500: loss 6.977751\n",
      "iteration 500 / 1500: loss 3.405880\n",
      "iteration 600 / 1500: loss 2.404778\n",
      "iteration 700 / 1500: loss 2.204826\n",
      "iteration 800 / 1500: loss 2.154643\n",
      "iteration 900 / 1500: loss 2.059559\n",
      "iteration 1000 / 1500: loss 2.108997\n",
      "iteration 1100 / 1500: loss 2.103134\n",
      "iteration 1200 / 1500: loss 2.118807\n",
      "iteration 1300 / 1500: loss 2.054259\n",
      "iteration 1400 / 1500: loss 2.121839\n",
      "iteration 0 / 1500: loss 1110.360978\n",
      "iteration 100 / 1500: loss 261.088229\n",
      "iteration 200 / 1500: loss 62.611360\n",
      "iteration 300 / 1500: loss 16.265388\n",
      "iteration 400 / 1500: loss 5.425920\n",
      "iteration 500 / 1500: loss 2.885023\n",
      "iteration 600 / 1500: loss 2.308695\n",
      "iteration 700 / 1500: loss 2.165760\n",
      "iteration 800 / 1500: loss 2.178243\n",
      "iteration 900 / 1500: loss 2.163684\n",
      "iteration 1000 / 1500: loss 2.144160\n",
      "iteration 1100 / 1500: loss 2.118334\n",
      "iteration 1200 / 1500: loss 2.144488\n",
      "iteration 1300 / 1500: loss 2.123273\n",
      "iteration 1400 / 1500: loss 2.061798\n",
      "iteration 0 / 1500: loss 1200.921918\n",
      "iteration 100 / 1500: loss 252.846948\n",
      "iteration 200 / 1500: loss 54.541584\n",
      "iteration 300 / 1500: loss 13.113815\n",
      "iteration 400 / 1500: loss 4.370278\n",
      "iteration 500 / 1500: loss 2.562405\n",
      "iteration 600 / 1500: loss 2.260353\n",
      "iteration 700 / 1500: loss 2.095037\n",
      "iteration 800 / 1500: loss 2.115723\n",
      "iteration 900 / 1500: loss 2.108043\n",
      "iteration 1000 / 1500: loss 2.195052\n",
      "iteration 1100 / 1500: loss 2.116355\n",
      "iteration 1200 / 1500: loss 2.137692\n",
      "iteration 1300 / 1500: loss 2.142448\n",
      "iteration 1400 / 1500: loss 2.102572\n",
      "iteration 0 / 1500: loss 1286.478668\n",
      "iteration 100 / 1500: loss 242.073879\n",
      "iteration 200 / 1500: loss 47.040877\n",
      "iteration 300 / 1500: loss 10.482634\n",
      "iteration 400 / 1500: loss 3.735582\n",
      "iteration 500 / 1500: loss 2.418598\n",
      "iteration 600 / 1500: loss 2.199594\n",
      "iteration 700 / 1500: loss 2.155410\n",
      "iteration 800 / 1500: loss 2.121883\n",
      "iteration 900 / 1500: loss 2.130522\n",
      "iteration 1000 / 1500: loss 2.110964\n",
      "iteration 1100 / 1500: loss 2.128198\n",
      "iteration 1200 / 1500: loss 2.122652\n",
      "iteration 1300 / 1500: loss 2.115499\n",
      "iteration 1400 / 1500: loss 2.132442\n",
      "iteration 0 / 1500: loss 1372.899930\n",
      "iteration 100 / 1500: loss 231.025462\n",
      "iteration 200 / 1500: loss 40.340165\n",
      "iteration 300 / 1500: loss 8.588080\n",
      "iteration 400 / 1500: loss 3.256569\n",
      "iteration 500 / 1500: loss 2.272888\n",
      "iteration 600 / 1500: loss 2.147441\n",
      "iteration 700 / 1500: loss 2.155043\n",
      "iteration 800 / 1500: loss 2.137601\n",
      "iteration 900 / 1500: loss 2.091971\n",
      "iteration 1000 / 1500: loss 2.082647\n",
      "iteration 1100 / 1500: loss 2.143322\n",
      "iteration 1200 / 1500: loss 2.133005\n",
      "iteration 1300 / 1500: loss 2.163935\n",
      "iteration 1400 / 1500: loss 2.145475\n",
      "iteration 0 / 1500: loss 1444.305643\n",
      "iteration 100 / 1500: loss 217.368858\n",
      "iteration 200 / 1500: loss 34.299578\n",
      "iteration 300 / 1500: loss 6.974028\n",
      "iteration 400 / 1500: loss 2.866939\n",
      "iteration 500 / 1500: loss 2.252012\n",
      "iteration 600 / 1500: loss 2.166531\n",
      "iteration 700 / 1500: loss 2.162924\n",
      "iteration 800 / 1500: loss 2.098376\n",
      "iteration 900 / 1500: loss 2.162272\n",
      "iteration 1000 / 1500: loss 2.141197\n",
      "iteration 1100 / 1500: loss 2.184555\n",
      "iteration 1200 / 1500: loss 2.154343\n",
      "iteration 1300 / 1500: loss 2.134573\n",
      "iteration 1400 / 1500: loss 2.146413\n",
      "iteration 0 / 1500: loss 1501.081420\n",
      "iteration 100 / 1500: loss 202.045247\n",
      "iteration 200 / 1500: loss 28.867489\n",
      "iteration 300 / 1500: loss 5.703167\n",
      "iteration 400 / 1500: loss 2.634078\n",
      "iteration 500 / 1500: loss 2.191014\n",
      "iteration 600 / 1500: loss 2.154125\n",
      "iteration 700 / 1500: loss 2.150358\n",
      "iteration 800 / 1500: loss 2.139275\n",
      "iteration 900 / 1500: loss 2.149959\n",
      "iteration 1000 / 1500: loss 2.146429\n",
      "iteration 1100 / 1500: loss 2.123178\n",
      "iteration 1200 / 1500: loss 2.151385\n",
      "iteration 1300 / 1500: loss 2.178180\n",
      "iteration 1400 / 1500: loss 2.101658\n",
      "iteration 0 / 1500: loss 767.840985\n",
      "iteration 100 / 1500: loss 180.666455\n",
      "iteration 200 / 1500: loss 43.886806\n",
      "iteration 300 / 1500: loss 11.905652\n",
      "iteration 400 / 1500: loss 4.343456\n",
      "iteration 500 / 1500: loss 2.654794\n",
      "iteration 600 / 1500: loss 2.143680\n",
      "iteration 700 / 1500: loss 2.116794\n",
      "iteration 800 / 1500: loss 2.130793\n",
      "iteration 900 / 1500: loss 2.062511\n",
      "iteration 1000 / 1500: loss 2.103563\n",
      "iteration 1100 / 1500: loss 2.115827\n",
      "iteration 1200 / 1500: loss 2.130986\n",
      "iteration 1300 / 1500: loss 2.003425\n",
      "iteration 1400 / 1500: loss 2.039949\n",
      "iteration 0 / 1500: loss 860.739183\n",
      "iteration 100 / 1500: loss 172.404309\n",
      "iteration 200 / 1500: loss 35.973768\n",
      "iteration 300 / 1500: loss 8.808650\n",
      "iteration 400 / 1500: loss 3.416103\n",
      "iteration 500 / 1500: loss 2.341985\n",
      "iteration 600 / 1500: loss 2.102634\n",
      "iteration 700 / 1500: loss 2.108556\n",
      "iteration 800 / 1500: loss 2.045401\n",
      "iteration 900 / 1500: loss 2.049160\n",
      "iteration 1000 / 1500: loss 2.076585\n",
      "iteration 1100 / 1500: loss 2.066581\n",
      "iteration 1200 / 1500: loss 2.075558\n",
      "iteration 1300 / 1500: loss 2.144628\n",
      "iteration 1400 / 1500: loss 2.106017\n",
      "iteration 0 / 1500: loss 967.535488\n",
      "iteration 100 / 1500: loss 165.025230\n",
      "iteration 200 / 1500: loss 29.677057\n",
      "iteration 300 / 1500: loss 6.769815\n",
      "iteration 400 / 1500: loss 2.912527\n",
      "iteration 500 / 1500: loss 2.285470\n",
      "iteration 600 / 1500: loss 2.182842\n",
      "iteration 700 / 1500: loss 2.062948\n",
      "iteration 800 / 1500: loss 2.070849\n",
      "iteration 900 / 1500: loss 2.095617\n",
      "iteration 1000 / 1500: loss 2.153154\n",
      "iteration 1100 / 1500: loss 2.071809\n",
      "iteration 1200 / 1500: loss 2.051359\n",
      "iteration 1300 / 1500: loss 2.091882\n",
      "iteration 1400 / 1500: loss 2.074800\n",
      "iteration 0 / 1500: loss 1033.849711\n",
      "iteration 100 / 1500: loss 150.323213\n",
      "iteration 200 / 1500: loss 23.444979\n",
      "iteration 300 / 1500: loss 5.191170\n",
      "iteration 400 / 1500: loss 2.591449\n",
      "iteration 500 / 1500: loss 2.155406\n",
      "iteration 600 / 1500: loss 2.129192\n",
      "iteration 700 / 1500: loss 2.123083\n",
      "iteration 800 / 1500: loss 2.056165\n",
      "iteration 900 / 1500: loss 2.092953\n",
      "iteration 1000 / 1500: loss 2.067819\n",
      "iteration 1100 / 1500: loss 2.056516\n",
      "iteration 1200 / 1500: loss 2.161014\n",
      "iteration 1300 / 1500: loss 2.059442\n",
      "iteration 1400 / 1500: loss 2.067723\n",
      "iteration 0 / 1500: loss 1117.584659\n",
      "iteration 100 / 1500: loss 138.406590\n",
      "iteration 200 / 1500: loss 18.806838\n",
      "iteration 300 / 1500: loss 4.117463\n",
      "iteration 400 / 1500: loss 2.373144\n",
      "iteration 500 / 1500: loss 2.146611\n",
      "iteration 600 / 1500: loss 2.115304\n",
      "iteration 700 / 1500: loss 2.134450\n",
      "iteration 800 / 1500: loss 2.080006\n",
      "iteration 900 / 1500: loss 2.073743\n",
      "iteration 1000 / 1500: loss 2.140994\n",
      "iteration 1100 / 1500: loss 2.174911\n",
      "iteration 1200 / 1500: loss 2.083723\n",
      "iteration 1300 / 1500: loss 2.100483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.162143\n",
      "iteration 0 / 1500: loss 1198.372062\n",
      "iteration 100 / 1500: loss 126.334429\n",
      "iteration 200 / 1500: loss 15.007794\n",
      "iteration 300 / 1500: loss 3.445400\n",
      "iteration 400 / 1500: loss 2.241308\n",
      "iteration 500 / 1500: loss 2.110052\n",
      "iteration 600 / 1500: loss 2.118446\n",
      "iteration 700 / 1500: loss 2.118449\n",
      "iteration 800 / 1500: loss 2.135151\n",
      "iteration 900 / 1500: loss 2.197127\n",
      "iteration 1000 / 1500: loss 2.069552\n",
      "iteration 1100 / 1500: loss 2.149016\n",
      "iteration 1200 / 1500: loss 2.117138\n",
      "iteration 1300 / 1500: loss 2.191928\n",
      "iteration 1400 / 1500: loss 2.141657\n",
      "iteration 0 / 1500: loss 1279.047056\n",
      "iteration 100 / 1500: loss 115.054645\n",
      "iteration 200 / 1500: loss 12.084309\n",
      "iteration 300 / 1500: loss 2.951672\n",
      "iteration 400 / 1500: loss 2.240744\n",
      "iteration 500 / 1500: loss 2.077714\n",
      "iteration 600 / 1500: loss 2.078399\n",
      "iteration 700 / 1500: loss 2.128630\n",
      "iteration 800 / 1500: loss 2.134962\n",
      "iteration 900 / 1500: loss 2.055975\n",
      "iteration 1000 / 1500: loss 2.106778\n",
      "iteration 1100 / 1500: loss 2.068457\n",
      "iteration 1200 / 1500: loss 2.078808\n",
      "iteration 1300 / 1500: loss 2.119529\n",
      "iteration 1400 / 1500: loss 2.144103\n",
      "iteration 0 / 1500: loss 1368.902563\n",
      "iteration 100 / 1500: loss 104.733400\n",
      "iteration 200 / 1500: loss 9.854584\n",
      "iteration 300 / 1500: loss 2.713666\n",
      "iteration 400 / 1500: loss 2.165203\n",
      "iteration 500 / 1500: loss 2.148915\n",
      "iteration 600 / 1500: loss 2.152557\n",
      "iteration 700 / 1500: loss 2.126567\n",
      "iteration 800 / 1500: loss 2.138550\n",
      "iteration 900 / 1500: loss 2.093807\n",
      "iteration 1000 / 1500: loss 2.144880\n",
      "iteration 1100 / 1500: loss 2.121082\n",
      "iteration 1200 / 1500: loss 2.135165\n",
      "iteration 1300 / 1500: loss 2.113114\n",
      "iteration 1400 / 1500: loss 2.137993\n",
      "iteration 0 / 1500: loss 1457.493892\n",
      "iteration 100 / 1500: loss 94.807473\n",
      "iteration 200 / 1500: loss 8.096178\n",
      "iteration 300 / 1500: loss 2.516289\n",
      "iteration 400 / 1500: loss 2.132308\n",
      "iteration 500 / 1500: loss 2.160933\n",
      "iteration 600 / 1500: loss 2.140981\n",
      "iteration 700 / 1500: loss 2.127622\n",
      "iteration 800 / 1500: loss 2.112412\n",
      "iteration 900 / 1500: loss 2.095258\n",
      "iteration 1000 / 1500: loss 2.151676\n",
      "iteration 1100 / 1500: loss 2.148029\n",
      "iteration 1200 / 1500: loss 2.149696\n",
      "iteration 1300 / 1500: loss 2.144554\n",
      "iteration 1400 / 1500: loss 2.135170\n",
      "iteration 0 / 1500: loss 1541.985473\n",
      "iteration 100 / 1500: loss 85.621355\n",
      "iteration 200 / 1500: loss 6.673598\n",
      "iteration 300 / 1500: loss 2.401077\n",
      "iteration 400 / 1500: loss 2.185933\n",
      "iteration 500 / 1500: loss 2.169443\n",
      "iteration 600 / 1500: loss 2.164675\n",
      "iteration 700 / 1500: loss 2.125179\n",
      "iteration 800 / 1500: loss 2.169633\n",
      "iteration 900 / 1500: loss 2.159930\n",
      "iteration 1000 / 1500: loss 2.178059\n",
      "iteration 1100 / 1500: loss 2.129402\n",
      "iteration 1200 / 1500: loss 2.151148\n",
      "iteration 1300 / 1500: loss 2.120568\n",
      "iteration 1400 / 1500: loss 2.139116\n",
      "iteration 0 / 1500: loss 782.752467\n",
      "iteration 100 / 1500: loss 118.158719\n",
      "iteration 200 / 1500: loss 19.402039\n",
      "iteration 300 / 1500: loss 4.649673\n",
      "iteration 400 / 1500: loss 2.486849\n",
      "iteration 500 / 1500: loss 2.236875\n",
      "iteration 600 / 1500: loss 2.103511\n",
      "iteration 700 / 1500: loss 2.049423\n",
      "iteration 800 / 1500: loss 2.096832\n",
      "iteration 900 / 1500: loss 2.070211\n",
      "iteration 1000 / 1500: loss 2.064103\n",
      "iteration 1100 / 1500: loss 2.129717\n",
      "iteration 1200 / 1500: loss 2.093424\n",
      "iteration 1300 / 1500: loss 2.054326\n",
      "iteration 1400 / 1500: loss 2.106372\n",
      "iteration 0 / 1500: loss 855.793936\n",
      "iteration 100 / 1500: loss 104.958387\n",
      "iteration 200 / 1500: loss 14.533140\n",
      "iteration 300 / 1500: loss 3.594120\n",
      "iteration 400 / 1500: loss 2.257554\n",
      "iteration 500 / 1500: loss 2.108524\n",
      "iteration 600 / 1500: loss 2.093164\n",
      "iteration 700 / 1500: loss 2.034524\n",
      "iteration 800 / 1500: loss 2.058926\n",
      "iteration 900 / 1500: loss 2.006867\n",
      "iteration 1000 / 1500: loss 2.112301\n",
      "iteration 1100 / 1500: loss 2.032456\n",
      "iteration 1200 / 1500: loss 2.084513\n",
      "iteration 1300 / 1500: loss 2.149905\n",
      "iteration 1400 / 1500: loss 2.107823\n",
      "iteration 0 / 1500: loss 932.344021\n",
      "iteration 100 / 1500: loss 92.623361\n",
      "iteration 200 / 1500: loss 10.926607\n",
      "iteration 300 / 1500: loss 2.962074\n",
      "iteration 400 / 1500: loss 2.218037\n",
      "iteration 500 / 1500: loss 2.124033\n",
      "iteration 600 / 1500: loss 2.082851\n",
      "iteration 700 / 1500: loss 2.110745\n",
      "iteration 800 / 1500: loss 2.114043\n",
      "iteration 900 / 1500: loss 2.092136\n",
      "iteration 1000 / 1500: loss 2.131930\n",
      "iteration 1100 / 1500: loss 2.144834\n",
      "iteration 1200 / 1500: loss 2.163960\n",
      "iteration 1300 / 1500: loss 2.125525\n",
      "iteration 1400 / 1500: loss 2.207159\n",
      "iteration 0 / 1500: loss 1031.303938\n",
      "iteration 100 / 1500: loss 83.259195\n",
      "iteration 200 / 1500: loss 8.532108\n",
      "iteration 300 / 1500: loss 2.628089\n",
      "iteration 400 / 1500: loss 2.220445\n",
      "iteration 500 / 1500: loss 2.144165\n",
      "iteration 600 / 1500: loss 2.079032\n",
      "iteration 700 / 1500: loss 2.129219\n",
      "iteration 800 / 1500: loss 2.124376\n",
      "iteration 900 / 1500: loss 2.101332\n",
      "iteration 1000 / 1500: loss 2.117921\n",
      "iteration 1100 / 1500: loss 2.100019\n",
      "iteration 1200 / 1500: loss 2.082440\n",
      "iteration 1300 / 1500: loss 2.157863\n",
      "iteration 1400 / 1500: loss 2.113650\n",
      "iteration 0 / 1500: loss 1118.009370\n",
      "iteration 100 / 1500: loss 73.239430\n",
      "iteration 200 / 1500: loss 6.631211\n",
      "iteration 300 / 1500: loss 2.407128\n",
      "iteration 400 / 1500: loss 2.116868\n",
      "iteration 500 / 1500: loss 2.088773\n",
      "iteration 600 / 1500: loss 2.182435\n",
      "iteration 700 / 1500: loss 2.095349\n",
      "iteration 800 / 1500: loss 2.170822\n",
      "iteration 900 / 1500: loss 2.122383\n",
      "iteration 1000 / 1500: loss 2.107926\n",
      "iteration 1100 / 1500: loss 2.121144\n",
      "iteration 1200 / 1500: loss 2.109897\n",
      "iteration 1300 / 1500: loss 2.137480\n",
      "iteration 1400 / 1500: loss 2.159685\n",
      "iteration 0 / 1500: loss 1201.401550\n",
      "iteration 100 / 1500: loss 63.857047\n",
      "iteration 200 / 1500: loss 5.325635\n",
      "iteration 300 / 1500: loss 2.286444\n",
      "iteration 400 / 1500: loss 2.101518\n",
      "iteration 500 / 1500: loss 2.124785\n",
      "iteration 600 / 1500: loss 2.188233\n",
      "iteration 700 / 1500: loss 2.107712\n",
      "iteration 800 / 1500: loss 2.144997\n",
      "iteration 900 / 1500: loss 2.116357\n",
      "iteration 1000 / 1500: loss 2.155681\n",
      "iteration 1100 / 1500: loss 2.107513\n",
      "iteration 1200 / 1500: loss 2.147929\n",
      "iteration 1300 / 1500: loss 2.069584\n",
      "iteration 1400 / 1500: loss 2.127502\n",
      "iteration 0 / 1500: loss 1291.939377\n",
      "iteration 100 / 1500: loss 55.854574\n",
      "iteration 200 / 1500: loss 4.348128\n",
      "iteration 300 / 1500: loss 2.225378\n",
      "iteration 400 / 1500: loss 2.117990\n",
      "iteration 500 / 1500: loss 2.106408\n",
      "iteration 600 / 1500: loss 2.116357\n",
      "iteration 700 / 1500: loss 2.136413\n",
      "iteration 800 / 1500: loss 2.171020\n",
      "iteration 900 / 1500: loss 2.081281\n",
      "iteration 1000 / 1500: loss 2.124961\n",
      "iteration 1100 / 1500: loss 2.167934\n",
      "iteration 1200 / 1500: loss 2.151142\n",
      "iteration 1300 / 1500: loss 2.097369\n",
      "iteration 1400 / 1500: loss 2.162764\n",
      "iteration 0 / 1500: loss 1379.470977\n",
      "iteration 100 / 1500: loss 48.387690\n",
      "iteration 200 / 1500: loss 3.643107\n",
      "iteration 300 / 1500: loss 2.201814\n",
      "iteration 400 / 1500: loss 2.129720\n",
      "iteration 500 / 1500: loss 2.102659\n",
      "iteration 600 / 1500: loss 2.127979\n",
      "iteration 700 / 1500: loss 2.101558\n",
      "iteration 800 / 1500: loss 2.115523\n",
      "iteration 900 / 1500: loss 2.152066\n",
      "iteration 1000 / 1500: loss 2.074454\n",
      "iteration 1100 / 1500: loss 2.109779\n",
      "iteration 1200 / 1500: loss 2.123939\n",
      "iteration 1300 / 1500: loss 2.126272\n",
      "iteration 1400 / 1500: loss 2.152027\n",
      "iteration 0 / 1500: loss 1449.843823\n",
      "iteration 100 / 1500: loss 41.472435\n",
      "iteration 200 / 1500: loss 3.218566\n",
      "iteration 300 / 1500: loss 2.143516\n",
      "iteration 400 / 1500: loss 2.162606\n",
      "iteration 500 / 1500: loss 2.113994\n",
      "iteration 600 / 1500: loss 2.137452\n",
      "iteration 700 / 1500: loss 2.135997\n",
      "iteration 800 / 1500: loss 2.102912\n",
      "iteration 900 / 1500: loss 2.170130\n",
      "iteration 1000 / 1500: loss 2.146189\n",
      "iteration 1100 / 1500: loss 2.166411\n",
      "iteration 1200 / 1500: loss 2.114318\n",
      "iteration 1300 / 1500: loss 2.116012\n",
      "iteration 1400 / 1500: loss 2.142029\n",
      "iteration 0 / 1500: loss 1547.552838\n",
      "iteration 100 / 1500: loss 36.063025\n",
      "iteration 200 / 1500: loss 2.909245\n",
      "iteration 300 / 1500: loss 2.136568\n",
      "iteration 400 / 1500: loss 2.177608\n",
      "iteration 500 / 1500: loss 2.170837\n",
      "iteration 600 / 1500: loss 2.159997\n",
      "iteration 700 / 1500: loss 2.152621\n",
      "iteration 800 / 1500: loss 2.164048\n",
      "iteration 900 / 1500: loss 2.081420\n",
      "iteration 1000 / 1500: loss 2.103049\n",
      "iteration 1100 / 1500: loss 2.139609\n",
      "iteration 1200 / 1500: loss 2.149362\n",
      "iteration 1300 / 1500: loss 2.208171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.146402\n",
      "iteration 0 / 1500: loss 762.501361\n",
      "iteration 100 / 1500: loss 74.186712\n",
      "iteration 200 / 1500: loss 8.910486\n",
      "iteration 300 / 1500: loss 2.693934\n",
      "iteration 400 / 1500: loss 2.133412\n",
      "iteration 500 / 1500: loss 2.069489\n",
      "iteration 600 / 1500: loss 2.049150\n",
      "iteration 700 / 1500: loss 2.070483\n",
      "iteration 800 / 1500: loss 2.077668\n",
      "iteration 900 / 1500: loss 2.101037\n",
      "iteration 1000 / 1500: loss 2.072031\n",
      "iteration 1100 / 1500: loss 2.104530\n",
      "iteration 1200 / 1500: loss 2.115237\n",
      "iteration 1300 / 1500: loss 2.065541\n",
      "iteration 1400 / 1500: loss 2.134494\n",
      "iteration 0 / 1500: loss 859.989895\n",
      "iteration 100 / 1500: loss 64.676518\n",
      "iteration 200 / 1500: loss 6.667509\n",
      "iteration 300 / 1500: loss 2.377587\n",
      "iteration 400 / 1500: loss 2.056525\n",
      "iteration 500 / 1500: loss 2.119887\n",
      "iteration 600 / 1500: loss 2.122142\n",
      "iteration 700 / 1500: loss 2.093198\n",
      "iteration 800 / 1500: loss 2.115625\n",
      "iteration 900 / 1500: loss 2.076468\n",
      "iteration 1000 / 1500: loss 2.116585\n",
      "iteration 1100 / 1500: loss 2.106637\n",
      "iteration 1200 / 1500: loss 2.107112\n",
      "iteration 1300 / 1500: loss 2.103678\n",
      "iteration 1400 / 1500: loss 2.105459\n",
      "iteration 0 / 1500: loss 964.312801\n",
      "iteration 100 / 1500: loss 56.079363\n",
      "iteration 200 / 1500: loss 5.206636\n",
      "iteration 300 / 1500: loss 2.279023\n",
      "iteration 400 / 1500: loss 2.119125\n",
      "iteration 500 / 1500: loss 2.066196\n",
      "iteration 600 / 1500: loss 2.168217\n",
      "iteration 700 / 1500: loss 2.080930\n",
      "iteration 800 / 1500: loss 2.097709\n",
      "iteration 900 / 1500: loss 2.121467\n",
      "iteration 1000 / 1500: loss 2.064047\n",
      "iteration 1100 / 1500: loss 2.127235\n",
      "iteration 1200 / 1500: loss 2.075368\n",
      "iteration 1300 / 1500: loss 2.150871\n",
      "iteration 1400 / 1500: loss 2.078151\n",
      "iteration 0 / 1500: loss 1025.273689\n",
      "iteration 100 / 1500: loss 46.224397\n",
      "iteration 200 / 1500: loss 4.030692\n",
      "iteration 300 / 1500: loss 2.159296\n",
      "iteration 400 / 1500: loss 2.143833\n",
      "iteration 500 / 1500: loss 2.143154\n",
      "iteration 600 / 1500: loss 2.177790\n",
      "iteration 700 / 1500: loss 2.154531\n",
      "iteration 800 / 1500: loss 2.098099\n",
      "iteration 900 / 1500: loss 2.104320\n",
      "iteration 1000 / 1500: loss 2.159247\n",
      "iteration 1100 / 1500: loss 2.063135\n",
      "iteration 1200 / 1500: loss 2.096159\n",
      "iteration 1300 / 1500: loss 2.113478\n",
      "iteration 1400 / 1500: loss 2.132514\n",
      "iteration 0 / 1500: loss 1122.149974\n",
      "iteration 100 / 1500: loss 39.237367\n",
      "iteration 200 / 1500: loss 3.303146\n",
      "iteration 300 / 1500: loss 2.174879\n",
      "iteration 400 / 1500: loss 2.164912\n",
      "iteration 500 / 1500: loss 2.099881\n",
      "iteration 600 / 1500: loss 2.105788\n",
      "iteration 700 / 1500: loss 2.090868\n",
      "iteration 800 / 1500: loss 2.091400\n",
      "iteration 900 / 1500: loss 2.077219\n",
      "iteration 1000 / 1500: loss 2.149928\n",
      "iteration 1100 / 1500: loss 2.088327\n",
      "iteration 1200 / 1500: loss 2.102101\n",
      "iteration 1300 / 1500: loss 2.096100\n",
      "iteration 1400 / 1500: loss 2.110347\n",
      "iteration 0 / 1500: loss 1197.980435\n",
      "iteration 100 / 1500: loss 32.520934\n",
      "iteration 200 / 1500: loss 2.881045\n",
      "iteration 300 / 1500: loss 2.076944\n",
      "iteration 400 / 1500: loss 2.144618\n",
      "iteration 500 / 1500: loss 2.117940\n",
      "iteration 600 / 1500: loss 2.081684\n",
      "iteration 700 / 1500: loss 2.116940\n",
      "iteration 800 / 1500: loss 2.100274\n",
      "iteration 900 / 1500: loss 2.133037\n",
      "iteration 1000 / 1500: loss 2.125433\n",
      "iteration 1100 / 1500: loss 2.072938\n",
      "iteration 1200 / 1500: loss 2.151356\n",
      "iteration 1300 / 1500: loss 2.085199\n",
      "iteration 1400 / 1500: loss 2.135471\n",
      "iteration 0 / 1500: loss 1279.742323\n",
      "iteration 100 / 1500: loss 27.132307\n",
      "iteration 200 / 1500: loss 2.594765\n",
      "iteration 300 / 1500: loss 2.115493\n",
      "iteration 400 / 1500: loss 2.082689\n",
      "iteration 500 / 1500: loss 2.076948\n",
      "iteration 600 / 1500: loss 2.105763\n",
      "iteration 700 / 1500: loss 2.151297\n",
      "iteration 800 / 1500: loss 2.108589\n",
      "iteration 900 / 1500: loss 2.132157\n",
      "iteration 1000 / 1500: loss 2.121261\n",
      "iteration 1100 / 1500: loss 2.108797\n",
      "iteration 1200 / 1500: loss 2.144044\n",
      "iteration 1300 / 1500: loss 2.146682\n",
      "iteration 1400 / 1500: loss 2.092423\n",
      "iteration 0 / 1500: loss 1372.143468\n",
      "iteration 100 / 1500: loss 22.660211\n",
      "iteration 200 / 1500: loss 2.462396\n",
      "iteration 300 / 1500: loss 2.180595\n",
      "iteration 400 / 1500: loss 2.127495\n",
      "iteration 500 / 1500: loss 2.153378\n",
      "iteration 600 / 1500: loss 2.121517\n",
      "iteration 700 / 1500: loss 2.085548\n",
      "iteration 800 / 1500: loss 2.160127\n",
      "iteration 900 / 1500: loss 2.138742\n",
      "iteration 1000 / 1500: loss 2.121467\n",
      "iteration 1100 / 1500: loss 2.126178\n",
      "iteration 1200 / 1500: loss 2.121533\n",
      "iteration 1300 / 1500: loss 2.118282\n",
      "iteration 1400 / 1500: loss 2.118627\n",
      "iteration 0 / 1500: loss 1460.315759\n",
      "iteration 100 / 1500: loss 18.828595\n",
      "iteration 200 / 1500: loss 2.373825\n",
      "iteration 300 / 1500: loss 2.167597\n",
      "iteration 400 / 1500: loss 2.141765\n",
      "iteration 500 / 1500: loss 2.173599\n",
      "iteration 600 / 1500: loss 2.147978\n",
      "iteration 700 / 1500: loss 2.144148\n",
      "iteration 800 / 1500: loss 2.168882\n",
      "iteration 900 / 1500: loss 2.099624\n",
      "iteration 1000 / 1500: loss 2.139111\n",
      "iteration 1100 / 1500: loss 2.129517\n",
      "iteration 1200 / 1500: loss 2.108987\n",
      "iteration 1300 / 1500: loss 2.093187\n",
      "iteration 1400 / 1500: loss 2.163746\n",
      "iteration 0 / 1500: loss 1529.435842\n",
      "iteration 100 / 1500: loss 15.596973\n",
      "iteration 200 / 1500: loss 2.288786\n",
      "iteration 300 / 1500: loss 2.109942\n",
      "iteration 400 / 1500: loss 2.131956\n",
      "iteration 500 / 1500: loss 2.141309\n",
      "iteration 600 / 1500: loss 2.168945\n",
      "iteration 700 / 1500: loss 2.175882\n",
      "iteration 800 / 1500: loss 2.136007\n",
      "iteration 900 / 1500: loss 2.114203\n",
      "iteration 1000 / 1500: loss 2.113819\n",
      "iteration 1100 / 1500: loss 2.109485\n",
      "iteration 1200 / 1500: loss 2.123978\n",
      "iteration 1300 / 1500: loss 2.153065\n",
      "iteration 1400 / 1500: loss 2.134527\n",
      "iteration 0 / 1500: loss 771.822678\n",
      "iteration 100 / 1500: loss 48.623979\n",
      "iteration 200 / 1500: loss 4.915997\n",
      "iteration 300 / 1500: loss 2.274450\n",
      "iteration 400 / 1500: loss 2.058971\n",
      "iteration 500 / 1500: loss 2.096465\n",
      "iteration 600 / 1500: loss 2.053709\n",
      "iteration 700 / 1500: loss 2.062704\n",
      "iteration 800 / 1500: loss 2.028044\n",
      "iteration 900 / 1500: loss 2.089994\n",
      "iteration 1000 / 1500: loss 2.026719\n",
      "iteration 1100 / 1500: loss 2.037469\n",
      "iteration 1200 / 1500: loss 2.062447\n",
      "iteration 1300 / 1500: loss 2.085867\n",
      "iteration 1400 / 1500: loss 2.128287\n",
      "iteration 0 / 1500: loss 867.958228\n",
      "iteration 100 / 1500: loss 40.291280\n",
      "iteration 200 / 1500: loss 3.830562\n",
      "iteration 300 / 1500: loss 2.128637\n",
      "iteration 400 / 1500: loss 2.090393\n",
      "iteration 500 / 1500: loss 2.067895\n",
      "iteration 600 / 1500: loss 2.058416\n",
      "iteration 700 / 1500: loss 2.050016\n",
      "iteration 800 / 1500: loss 2.095329\n",
      "iteration 900 / 1500: loss 2.091005\n",
      "iteration 1000 / 1500: loss 2.163607\n",
      "iteration 1100 / 1500: loss 2.123581\n",
      "iteration 1200 / 1500: loss 2.141643\n",
      "iteration 1300 / 1500: loss 2.092248\n",
      "iteration 1400 / 1500: loss 2.190525\n",
      "iteration 0 / 1500: loss 939.146970\n",
      "iteration 100 / 1500: loss 32.313746\n",
      "iteration 200 / 1500: loss 3.108335\n",
      "iteration 300 / 1500: loss 2.129986\n",
      "iteration 400 / 1500: loss 2.099885\n",
      "iteration 500 / 1500: loss 2.062129\n",
      "iteration 600 / 1500: loss 2.133332\n",
      "iteration 700 / 1500: loss 2.121845\n",
      "iteration 800 / 1500: loss 2.114133\n",
      "iteration 900 / 1500: loss 2.156677\n",
      "iteration 1000 / 1500: loss 2.120361\n",
      "iteration 1100 / 1500: loss 2.122656\n",
      "iteration 1200 / 1500: loss 2.147059\n",
      "iteration 1300 / 1500: loss 2.128206\n",
      "iteration 1400 / 1500: loss 2.114535\n",
      "iteration 0 / 1500: loss 1029.735579\n",
      "iteration 100 / 1500: loss 26.347401\n",
      "iteration 200 / 1500: loss 2.705201\n",
      "iteration 300 / 1500: loss 2.095579\n",
      "iteration 400 / 1500: loss 2.124872\n",
      "iteration 500 / 1500: loss 2.102533\n",
      "iteration 600 / 1500: loss 2.101266\n",
      "iteration 700 / 1500: loss 2.060181\n",
      "iteration 800 / 1500: loss 2.114574\n",
      "iteration 900 / 1500: loss 2.151941\n",
      "iteration 1000 / 1500: loss 2.093335\n",
      "iteration 1100 / 1500: loss 2.114168\n",
      "iteration 1200 / 1500: loss 2.164932\n",
      "iteration 1300 / 1500: loss 2.147744\n",
      "iteration 1400 / 1500: loss 2.136418\n",
      "iteration 0 / 1500: loss 1098.568640\n",
      "iteration 100 / 1500: loss 20.984808\n",
      "iteration 200 / 1500: loss 2.471133\n",
      "iteration 300 / 1500: loss 2.110907\n",
      "iteration 400 / 1500: loss 2.111561\n",
      "iteration 500 / 1500: loss 2.132566\n",
      "iteration 600 / 1500: loss 2.112501\n",
      "iteration 700 / 1500: loss 2.107733\n",
      "iteration 800 / 1500: loss 2.169872\n",
      "iteration 900 / 1500: loss 2.131871\n",
      "iteration 1000 / 1500: loss 2.173266\n",
      "iteration 1100 / 1500: loss 2.093003\n",
      "iteration 1200 / 1500: loss 2.135583\n",
      "iteration 1300 / 1500: loss 2.145491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.119924\n",
      "iteration 0 / 1500: loss 1202.918395\n",
      "iteration 100 / 1500: loss 17.233392\n",
      "iteration 200 / 1500: loss 2.333311\n",
      "iteration 300 / 1500: loss 2.085897\n",
      "iteration 400 / 1500: loss 2.117423\n",
      "iteration 500 / 1500: loss 2.146587\n",
      "iteration 600 / 1500: loss 2.153595\n",
      "iteration 700 / 1500: loss 2.126848\n",
      "iteration 800 / 1500: loss 2.126507\n",
      "iteration 900 / 1500: loss 2.130889\n",
      "iteration 1000 / 1500: loss 2.108931\n",
      "iteration 1100 / 1500: loss 2.121541\n",
      "iteration 1200 / 1500: loss 2.103217\n",
      "iteration 1300 / 1500: loss 2.093331\n",
      "iteration 1400 / 1500: loss 2.169052\n",
      "iteration 0 / 1500: loss 1297.313030\n",
      "iteration 100 / 1500: loss 13.997145\n",
      "iteration 200 / 1500: loss 2.289821\n",
      "iteration 300 / 1500: loss 2.132528\n",
      "iteration 400 / 1500: loss 2.132907\n",
      "iteration 500 / 1500: loss 2.122375\n",
      "iteration 600 / 1500: loss 2.099673\n",
      "iteration 700 / 1500: loss 2.122716\n",
      "iteration 800 / 1500: loss 2.089907\n",
      "iteration 900 / 1500: loss 2.113045\n",
      "iteration 1000 / 1500: loss 2.160133\n",
      "iteration 1100 / 1500: loss 2.126551\n",
      "iteration 1200 / 1500: loss 2.163643\n",
      "iteration 1300 / 1500: loss 2.183528\n",
      "iteration 1400 / 1500: loss 2.142635\n",
      "iteration 0 / 1500: loss 1376.318012\n",
      "iteration 100 / 1500: loss 11.308633\n",
      "iteration 200 / 1500: loss 2.217779\n",
      "iteration 300 / 1500: loss 2.081695\n",
      "iteration 400 / 1500: loss 2.176266\n",
      "iteration 500 / 1500: loss 2.141098\n",
      "iteration 600 / 1500: loss 2.148565\n",
      "iteration 700 / 1500: loss 2.135170\n",
      "iteration 800 / 1500: loss 2.124712\n",
      "iteration 900 / 1500: loss 2.136650\n",
      "iteration 1000 / 1500: loss 2.108390\n",
      "iteration 1100 / 1500: loss 2.134300\n",
      "iteration 1200 / 1500: loss 2.117923\n",
      "iteration 1300 / 1500: loss 2.109999\n",
      "iteration 1400 / 1500: loss 2.105480\n",
      "iteration 0 / 1500: loss 1470.865842\n",
      "iteration 100 / 1500: loss 9.326908\n",
      "iteration 200 / 1500: loss 2.190697\n",
      "iteration 300 / 1500: loss 2.095774\n",
      "iteration 400 / 1500: loss 2.145331\n",
      "iteration 500 / 1500: loss 2.162055\n",
      "iteration 600 / 1500: loss 2.155169\n",
      "iteration 700 / 1500: loss 2.180171\n",
      "iteration 800 / 1500: loss 2.147136\n",
      "iteration 900 / 1500: loss 2.143006\n",
      "iteration 1000 / 1500: loss 2.149329\n",
      "iteration 1100 / 1500: loss 2.132776\n",
      "iteration 1200 / 1500: loss 2.172765\n",
      "iteration 1300 / 1500: loss 2.103931\n",
      "iteration 1400 / 1500: loss 2.157838\n",
      "iteration 0 / 1500: loss 1531.343897\n",
      "iteration 100 / 1500: loss 7.566113\n",
      "iteration 200 / 1500: loss 2.125410\n",
      "iteration 300 / 1500: loss 2.138961\n",
      "iteration 400 / 1500: loss 2.089410\n",
      "iteration 500 / 1500: loss 2.154133\n",
      "iteration 600 / 1500: loss 2.138131\n",
      "iteration 700 / 1500: loss 2.129176\n",
      "iteration 800 / 1500: loss 2.131090\n",
      "iteration 900 / 1500: loss 2.109521\n",
      "iteration 1000 / 1500: loss 2.158274\n",
      "iteration 1100 / 1500: loss 2.144973\n",
      "iteration 1200 / 1500: loss 2.172324\n",
      "iteration 1300 / 1500: loss 2.159514\n",
      "iteration 1400 / 1500: loss 2.161276\n",
      "iteration 0 / 1500: loss 777.477225\n",
      "iteration 100 / 1500: loss 31.915019\n",
      "iteration 200 / 1500: loss 3.203192\n",
      "iteration 300 / 1500: loss 2.131665\n",
      "iteration 400 / 1500: loss 2.058249\n",
      "iteration 500 / 1500: loss 2.124521\n",
      "iteration 600 / 1500: loss 2.098533\n",
      "iteration 700 / 1500: loss 2.041664\n",
      "iteration 800 / 1500: loss 2.093392\n",
      "iteration 900 / 1500: loss 2.058069\n",
      "iteration 1000 / 1500: loss 2.110593\n",
      "iteration 1100 / 1500: loss 2.076770\n",
      "iteration 1200 / 1500: loss 2.081789\n",
      "iteration 1300 / 1500: loss 2.104333\n",
      "iteration 1400 / 1500: loss 2.036438\n",
      "iteration 0 / 1500: loss 864.885616\n",
      "iteration 100 / 1500: loss 25.167424\n",
      "iteration 200 / 1500: loss 2.717786\n",
      "iteration 300 / 1500: loss 2.100534\n",
      "iteration 400 / 1500: loss 2.108759\n",
      "iteration 500 / 1500: loss 2.100854\n",
      "iteration 600 / 1500: loss 2.071772\n",
      "iteration 700 / 1500: loss 2.107417\n",
      "iteration 800 / 1500: loss 2.145952\n",
      "iteration 900 / 1500: loss 2.088917\n",
      "iteration 1000 / 1500: loss 2.050285\n",
      "iteration 1100 / 1500: loss 2.062375\n",
      "iteration 1200 / 1500: loss 2.156528\n",
      "iteration 1300 / 1500: loss 2.078034\n",
      "iteration 1400 / 1500: loss 2.108261\n",
      "iteration 0 / 1500: loss 959.813628\n",
      "iteration 100 / 1500: loss 19.823838\n",
      "iteration 200 / 1500: loss 2.430872\n",
      "iteration 300 / 1500: loss 2.063363\n",
      "iteration 400 / 1500: loss 2.185603\n",
      "iteration 500 / 1500: loss 2.057787\n",
      "iteration 600 / 1500: loss 2.078285\n",
      "iteration 700 / 1500: loss 2.105401\n",
      "iteration 800 / 1500: loss 2.139825\n",
      "iteration 900 / 1500: loss 2.132214\n",
      "iteration 1000 / 1500: loss 2.143318\n",
      "iteration 1100 / 1500: loss 2.087388\n",
      "iteration 1200 / 1500: loss 2.142886\n",
      "iteration 1300 / 1500: loss 2.117375\n",
      "iteration 1400 / 1500: loss 2.139971\n",
      "iteration 0 / 1500: loss 1029.019284\n",
      "iteration 100 / 1500: loss 15.331495\n",
      "iteration 200 / 1500: loss 2.267791\n",
      "iteration 300 / 1500: loss 2.058696\n",
      "iteration 400 / 1500: loss 2.129204\n",
      "iteration 500 / 1500: loss 2.136170\n",
      "iteration 600 / 1500: loss 2.139798\n",
      "iteration 700 / 1500: loss 2.113636\n",
      "iteration 800 / 1500: loss 2.094188\n",
      "iteration 900 / 1500: loss 2.118214\n",
      "iteration 1000 / 1500: loss 2.096678\n",
      "iteration 1100 / 1500: loss 2.085088\n",
      "iteration 1200 / 1500: loss 2.102681\n",
      "iteration 1300 / 1500: loss 2.074747\n",
      "iteration 1400 / 1500: loss 2.118953\n",
      "iteration 0 / 1500: loss 1108.193059\n",
      "iteration 100 / 1500: loss 11.966553\n",
      "iteration 200 / 1500: loss 2.188600\n",
      "iteration 300 / 1500: loss 2.087498\n",
      "iteration 400 / 1500: loss 2.123953\n",
      "iteration 500 / 1500: loss 2.100893\n",
      "iteration 600 / 1500: loss 2.144872\n",
      "iteration 700 / 1500: loss 2.133752\n",
      "iteration 800 / 1500: loss 2.098630\n",
      "iteration 900 / 1500: loss 2.178516\n",
      "iteration 1000 / 1500: loss 2.123271\n",
      "iteration 1100 / 1500: loss 2.078126\n",
      "iteration 1200 / 1500: loss 2.117236\n",
      "iteration 1300 / 1500: loss 2.095886\n",
      "iteration 1400 / 1500: loss 2.139100\n",
      "iteration 0 / 1500: loss 1203.215107\n",
      "iteration 100 / 1500: loss 9.512123\n",
      "iteration 200 / 1500: loss 2.175462\n",
      "iteration 300 / 1500: loss 2.153629\n",
      "iteration 400 / 1500: loss 2.137061\n",
      "iteration 500 / 1500: loss 2.120846\n",
      "iteration 600 / 1500: loss 2.146310\n",
      "iteration 700 / 1500: loss 2.147105\n",
      "iteration 800 / 1500: loss 2.076167\n",
      "iteration 900 / 1500: loss 2.148143\n",
      "iteration 1000 / 1500: loss 2.095558\n",
      "iteration 1100 / 1500: loss 2.104797\n",
      "iteration 1200 / 1500: loss 2.103147\n",
      "iteration 1300 / 1500: loss 2.113770\n",
      "iteration 1400 / 1500: loss 2.101068\n",
      "iteration 0 / 1500: loss 1283.125267\n",
      "iteration 100 / 1500: loss 7.569621\n",
      "iteration 200 / 1500: loss 2.196410\n",
      "iteration 300 / 1500: loss 2.159029\n",
      "iteration 400 / 1500: loss 2.106956\n",
      "iteration 500 / 1500: loss 2.152474\n",
      "iteration 600 / 1500: loss 2.158506\n",
      "iteration 700 / 1500: loss 2.123535\n",
      "iteration 800 / 1500: loss 2.129085\n",
      "iteration 900 / 1500: loss 2.086400\n",
      "iteration 1000 / 1500: loss 2.126261\n",
      "iteration 1100 / 1500: loss 2.131307\n",
      "iteration 1200 / 1500: loss 2.147669\n",
      "iteration 1300 / 1500: loss 2.170114\n",
      "iteration 1400 / 1500: loss 2.147438\n",
      "iteration 0 / 1500: loss 1380.272640\n",
      "iteration 100 / 1500: loss 6.258052\n",
      "iteration 200 / 1500: loss 2.217834\n",
      "iteration 300 / 1500: loss 2.161479\n",
      "iteration 400 / 1500: loss 2.172768\n",
      "iteration 500 / 1500: loss 2.136728\n",
      "iteration 600 / 1500: loss 2.113687\n",
      "iteration 700 / 1500: loss 2.125075\n",
      "iteration 800 / 1500: loss 2.133726\n",
      "iteration 900 / 1500: loss 2.125401\n",
      "iteration 1000 / 1500: loss 2.107182\n",
      "iteration 1100 / 1500: loss 2.074040\n",
      "iteration 1200 / 1500: loss 2.138586\n",
      "iteration 1300 / 1500: loss 2.120696\n",
      "iteration 1400 / 1500: loss 2.116963\n",
      "iteration 0 / 1500: loss 1470.467712\n",
      "iteration 100 / 1500: loss 5.144321\n",
      "iteration 200 / 1500: loss 2.181081\n",
      "iteration 300 / 1500: loss 2.147708\n",
      "iteration 400 / 1500: loss 2.153389\n",
      "iteration 500 / 1500: loss 2.122777\n",
      "iteration 600 / 1500: loss 2.159066\n",
      "iteration 700 / 1500: loss 2.147567\n",
      "iteration 800 / 1500: loss 2.126475\n",
      "iteration 900 / 1500: loss 2.070480\n",
      "iteration 1000 / 1500: loss 2.183982\n",
      "iteration 1100 / 1500: loss 2.171287\n",
      "iteration 1200 / 1500: loss 2.136205\n",
      "iteration 1300 / 1500: loss 2.144090\n",
      "iteration 1400 / 1500: loss 2.143887\n",
      "iteration 0 / 1500: loss 1523.094762\n",
      "iteration 100 / 1500: loss 4.299633\n",
      "iteration 200 / 1500: loss 2.165316\n",
      "iteration 300 / 1500: loss 2.147210\n",
      "iteration 400 / 1500: loss 2.130399\n",
      "iteration 500 / 1500: loss 2.179462\n",
      "iteration 600 / 1500: loss 2.146659\n",
      "iteration 700 / 1500: loss 2.107088\n",
      "iteration 800 / 1500: loss 2.141322\n",
      "iteration 900 / 1500: loss 2.166316\n",
      "iteration 1000 / 1500: loss 2.162750\n",
      "iteration 1100 / 1500: loss 2.143657\n",
      "iteration 1200 / 1500: loss 2.142210\n",
      "iteration 1300 / 1500: loss 2.128047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.153297\n",
      "iteration 0 / 1500: loss 774.824096\n",
      "iteration 100 / 1500: loss 20.961554\n",
      "iteration 200 / 1500: loss 2.601116\n",
      "iteration 300 / 1500: loss 2.106345\n",
      "iteration 400 / 1500: loss 2.075293\n",
      "iteration 500 / 1500: loss 2.105878\n",
      "iteration 600 / 1500: loss 2.118320\n",
      "iteration 700 / 1500: loss 2.091522\n",
      "iteration 800 / 1500: loss 2.113946\n",
      "iteration 900 / 1500: loss 2.167941\n",
      "iteration 1000 / 1500: loss 2.065404\n",
      "iteration 1100 / 1500: loss 2.084410\n",
      "iteration 1200 / 1500: loss 2.097713\n",
      "iteration 1300 / 1500: loss 2.092803\n",
      "iteration 1400 / 1500: loss 2.047678\n",
      "iteration 0 / 1500: loss 850.829568\n",
      "iteration 100 / 1500: loss 15.813294\n",
      "iteration 200 / 1500: loss 2.321876\n",
      "iteration 300 / 1500: loss 2.108182\n",
      "iteration 400 / 1500: loss 2.072205\n",
      "iteration 500 / 1500: loss 2.104783\n",
      "iteration 600 / 1500: loss 2.046002\n",
      "iteration 700 / 1500: loss 2.124879\n",
      "iteration 800 / 1500: loss 2.171888\n",
      "iteration 900 / 1500: loss 2.102942\n",
      "iteration 1000 / 1500: loss 2.071045\n",
      "iteration 1100 / 1500: loss 2.100000\n",
      "iteration 1200 / 1500: loss 2.147206\n",
      "iteration 1300 / 1500: loss 2.135300\n",
      "iteration 1400 / 1500: loss 2.047974\n",
      "iteration 0 / 1500: loss 951.047771\n",
      "iteration 100 / 1500: loss 12.192788\n",
      "iteration 200 / 1500: loss 2.271985\n",
      "iteration 300 / 1500: loss 2.171470\n",
      "iteration 400 / 1500: loss 2.079734\n",
      "iteration 500 / 1500: loss 2.050457\n",
      "iteration 600 / 1500: loss 2.151613\n",
      "iteration 700 / 1500: loss 2.113903\n",
      "iteration 800 / 1500: loss 2.123581\n",
      "iteration 900 / 1500: loss 2.087114\n",
      "iteration 1000 / 1500: loss 2.151247\n",
      "iteration 1100 / 1500: loss 2.156057\n",
      "iteration 1200 / 1500: loss 1.999019\n",
      "iteration 1300 / 1500: loss 2.122543\n",
      "iteration 1400 / 1500: loss 2.072705\n",
      "iteration 0 / 1500: loss 1040.135998\n",
      "iteration 100 / 1500: loss 9.365327\n",
      "iteration 200 / 1500: loss 2.117212\n",
      "iteration 300 / 1500: loss 2.143324\n",
      "iteration 400 / 1500: loss 2.046717\n",
      "iteration 500 / 1500: loss 2.120730\n",
      "iteration 600 / 1500: loss 2.125803\n",
      "iteration 700 / 1500: loss 2.053633\n",
      "iteration 800 / 1500: loss 2.100128\n",
      "iteration 900 / 1500: loss 2.088037\n",
      "iteration 1000 / 1500: loss 2.100766\n",
      "iteration 1100 / 1500: loss 2.135726\n",
      "iteration 1200 / 1500: loss 2.186926\n",
      "iteration 1300 / 1500: loss 2.107073\n",
      "iteration 1400 / 1500: loss 2.089157\n",
      "iteration 0 / 1500: loss 1126.070931\n",
      "iteration 100 / 1500: loss 7.282755\n",
      "iteration 200 / 1500: loss 2.133527\n",
      "iteration 300 / 1500: loss 2.111816\n",
      "iteration 400 / 1500: loss 2.031299\n",
      "iteration 500 / 1500: loss 2.103650\n",
      "iteration 600 / 1500: loss 2.169109\n",
      "iteration 700 / 1500: loss 2.135646\n",
      "iteration 800 / 1500: loss 2.129275\n",
      "iteration 900 / 1500: loss 2.157044\n",
      "iteration 1000 / 1500: loss 2.121051\n",
      "iteration 1100 / 1500: loss 2.145270\n",
      "iteration 1200 / 1500: loss 2.116796\n",
      "iteration 1300 / 1500: loss 2.113008\n",
      "iteration 1400 / 1500: loss 2.138120\n",
      "iteration 0 / 1500: loss 1191.395740\n",
      "iteration 100 / 1500: loss 5.745433\n",
      "iteration 200 / 1500: loss 2.121637\n",
      "iteration 300 / 1500: loss 2.140960\n",
      "iteration 400 / 1500: loss 2.166869\n",
      "iteration 500 / 1500: loss 2.096968\n",
      "iteration 600 / 1500: loss 2.133886\n",
      "iteration 700 / 1500: loss 2.119896\n",
      "iteration 800 / 1500: loss 2.106891\n",
      "iteration 900 / 1500: loss 2.104636\n",
      "iteration 1000 / 1500: loss 2.116659\n",
      "iteration 1100 / 1500: loss 2.144879\n",
      "iteration 1200 / 1500: loss 2.090153\n",
      "iteration 1300 / 1500: loss 2.148691\n",
      "iteration 1400 / 1500: loss 2.153941\n",
      "iteration 0 / 1500: loss 1285.455787\n",
      "iteration 100 / 1500: loss 4.658835\n",
      "iteration 200 / 1500: loss 2.156698\n",
      "iteration 300 / 1500: loss 2.120150\n",
      "iteration 400 / 1500: loss 2.152688\n",
      "iteration 500 / 1500: loss 2.133164\n",
      "iteration 600 / 1500: loss 2.161546\n",
      "iteration 700 / 1500: loss 2.163305\n",
      "iteration 800 / 1500: loss 2.137940\n",
      "iteration 900 / 1500: loss 2.150187\n",
      "iteration 1000 / 1500: loss 2.065316\n",
      "iteration 1100 / 1500: loss 2.123886\n",
      "iteration 1200 / 1500: loss 2.100764\n",
      "iteration 1300 / 1500: loss 2.057712\n",
      "iteration 1400 / 1500: loss 2.134105\n",
      "iteration 0 / 1500: loss 1376.202032\n",
      "iteration 100 / 1500: loss 3.896677\n",
      "iteration 200 / 1500: loss 2.176258\n",
      "iteration 300 / 1500: loss 2.081610\n",
      "iteration 400 / 1500: loss 2.191767\n",
      "iteration 500 / 1500: loss 2.090459\n",
      "iteration 600 / 1500: loss 2.125634\n",
      "iteration 700 / 1500: loss 2.121689\n",
      "iteration 800 / 1500: loss 2.179887\n",
      "iteration 900 / 1500: loss 2.114570\n",
      "iteration 1000 / 1500: loss 2.142129\n",
      "iteration 1100 / 1500: loss 2.146069\n",
      "iteration 1200 / 1500: loss 2.120462\n",
      "iteration 1300 / 1500: loss 2.127017\n",
      "iteration 1400 / 1500: loss 2.150754\n",
      "iteration 0 / 1500: loss 1493.628242\n",
      "iteration 100 / 1500: loss 3.422664\n",
      "iteration 200 / 1500: loss 2.117073\n",
      "iteration 300 / 1500: loss 2.113801\n",
      "iteration 400 / 1500: loss 2.143393\n",
      "iteration 500 / 1500: loss 2.136691\n",
      "iteration 600 / 1500: loss 2.095072\n",
      "iteration 700 / 1500: loss 2.128194\n",
      "iteration 800 / 1500: loss 2.134111\n",
      "iteration 900 / 1500: loss 2.170560\n",
      "iteration 1000 / 1500: loss 2.169596\n",
      "iteration 1100 / 1500: loss 2.161875\n",
      "iteration 1200 / 1500: loss 2.112384\n",
      "iteration 1300 / 1500: loss 2.147982\n",
      "iteration 1400 / 1500: loss 2.147222\n",
      "iteration 0 / 1500: loss 1518.559558\n",
      "iteration 100 / 1500: loss 2.973445\n",
      "iteration 200 / 1500: loss 2.193353\n",
      "iteration 300 / 1500: loss 2.178681\n",
      "iteration 400 / 1500: loss 2.115348\n",
      "iteration 500 / 1500: loss 2.128542\n",
      "iteration 600 / 1500: loss 2.134172\n",
      "iteration 700 / 1500: loss 2.137710\n",
      "iteration 800 / 1500: loss 2.119746\n",
      "iteration 900 / 1500: loss 2.152504\n",
      "iteration 1000 / 1500: loss 2.186163\n",
      "iteration 1100 / 1500: loss 2.159747\n",
      "iteration 1200 / 1500: loss 2.121243\n",
      "iteration 1300 / 1500: loss 2.189489\n",
      "iteration 1400 / 1500: loss 2.143196\n",
      "iteration 0 / 1500: loss 775.784768\n",
      "iteration 100 / 1500: loss 14.069407\n",
      "iteration 200 / 1500: loss 2.326316\n",
      "iteration 300 / 1500: loss 2.082308\n",
      "iteration 400 / 1500: loss 2.108426\n",
      "iteration 500 / 1500: loss 2.049932\n",
      "iteration 600 / 1500: loss 2.133349\n",
      "iteration 700 / 1500: loss 2.087314\n",
      "iteration 800 / 1500: loss 2.088593\n",
      "iteration 900 / 1500: loss 2.101719\n",
      "iteration 1000 / 1500: loss 2.081142\n",
      "iteration 1100 / 1500: loss 2.112763\n",
      "iteration 1200 / 1500: loss 2.084325\n",
      "iteration 1300 / 1500: loss 2.030291\n",
      "iteration 1400 / 1500: loss 2.073813\n",
      "iteration 0 / 1500: loss 856.941350\n",
      "iteration 100 / 1500: loss 10.383449\n",
      "iteration 200 / 1500: loss 2.149451\n",
      "iteration 300 / 1500: loss 2.138939\n",
      "iteration 400 / 1500: loss 2.147722\n",
      "iteration 500 / 1500: loss 2.040036\n",
      "iteration 600 / 1500: loss 2.089782\n",
      "iteration 700 / 1500: loss 2.094768\n",
      "iteration 800 / 1500: loss 2.103597\n",
      "iteration 900 / 1500: loss 2.087434\n",
      "iteration 1000 / 1500: loss 2.093988\n",
      "iteration 1100 / 1500: loss 2.098221\n",
      "iteration 1200 / 1500: loss 2.100680\n",
      "iteration 1300 / 1500: loss 2.127521\n",
      "iteration 1400 / 1500: loss 2.140938\n",
      "iteration 0 / 1500: loss 942.391865\n",
      "iteration 100 / 1500: loss 7.838291\n",
      "iteration 200 / 1500: loss 2.118477\n",
      "iteration 300 / 1500: loss 2.093117\n",
      "iteration 400 / 1500: loss 2.056229\n",
      "iteration 500 / 1500: loss 2.131095\n",
      "iteration 600 / 1500: loss 2.150825\n",
      "iteration 700 / 1500: loss 2.166231\n",
      "iteration 800 / 1500: loss 2.160848\n",
      "iteration 900 / 1500: loss 2.067211\n",
      "iteration 1000 / 1500: loss 2.113510\n",
      "iteration 1100 / 1500: loss 2.098132\n",
      "iteration 1200 / 1500: loss 2.091378\n",
      "iteration 1300 / 1500: loss 2.144888\n",
      "iteration 1400 / 1500: loss 2.079599\n",
      "iteration 0 / 1500: loss 1032.633883\n",
      "iteration 100 / 1500: loss 6.014891\n",
      "iteration 200 / 1500: loss 2.120224\n",
      "iteration 300 / 1500: loss 2.120786\n",
      "iteration 400 / 1500: loss 2.097978\n",
      "iteration 500 / 1500: loss 2.111632\n",
      "iteration 600 / 1500: loss 2.087311\n",
      "iteration 700 / 1500: loss 2.116591\n",
      "iteration 800 / 1500: loss 2.120939\n",
      "iteration 900 / 1500: loss 2.072293\n",
      "iteration 1000 / 1500: loss 2.057353\n",
      "iteration 1100 / 1500: loss 2.088552\n",
      "iteration 1200 / 1500: loss 2.122901\n",
      "iteration 1300 / 1500: loss 2.104882\n",
      "iteration 1400 / 1500: loss 2.121794\n",
      "iteration 0 / 1500: loss 1114.720481\n",
      "iteration 100 / 1500: loss 4.779137\n",
      "iteration 200 / 1500: loss 2.125254\n",
      "iteration 300 / 1500: loss 2.116593\n",
      "iteration 400 / 1500: loss 2.146231\n",
      "iteration 500 / 1500: loss 2.183045\n",
      "iteration 600 / 1500: loss 2.093631\n",
      "iteration 700 / 1500: loss 2.023002\n",
      "iteration 800 / 1500: loss 2.108315\n",
      "iteration 900 / 1500: loss 2.080235\n",
      "iteration 1000 / 1500: loss 2.147683\n",
      "iteration 1100 / 1500: loss 2.139832\n",
      "iteration 1200 / 1500: loss 2.145486\n",
      "iteration 1300 / 1500: loss 2.121177\n",
      "iteration 1400 / 1500: loss 2.090628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 1195.765938\n",
      "iteration 100 / 1500: loss 3.873369\n",
      "iteration 200 / 1500: loss 2.109476\n",
      "iteration 300 / 1500: loss 2.158002\n",
      "iteration 400 / 1500: loss 2.133964\n",
      "iteration 500 / 1500: loss 2.146266\n",
      "iteration 600 / 1500: loss 2.177712\n",
      "iteration 700 / 1500: loss 2.134845\n",
      "iteration 800 / 1500: loss 2.140493\n",
      "iteration 900 / 1500: loss 2.103361\n",
      "iteration 1000 / 1500: loss 2.147292\n",
      "iteration 1100 / 1500: loss 2.122577\n",
      "iteration 1200 / 1500: loss 2.148941\n",
      "iteration 1300 / 1500: loss 2.135003\n",
      "iteration 1400 / 1500: loss 2.112740\n",
      "iteration 0 / 1500: loss 1278.838996\n",
      "iteration 100 / 1500: loss 3.309242\n",
      "iteration 200 / 1500: loss 2.124652\n",
      "iteration 300 / 1500: loss 2.127142\n",
      "iteration 400 / 1500: loss 2.139882\n",
      "iteration 500 / 1500: loss 2.102508\n",
      "iteration 600 / 1500: loss 2.139709\n",
      "iteration 700 / 1500: loss 2.140608\n",
      "iteration 800 / 1500: loss 2.101387\n",
      "iteration 900 / 1500: loss 2.081677\n",
      "iteration 1000 / 1500: loss 2.137670\n",
      "iteration 1100 / 1500: loss 2.105759\n",
      "iteration 1200 / 1500: loss 2.187694\n",
      "iteration 1300 / 1500: loss 2.103777\n",
      "iteration 1400 / 1500: loss 2.147063\n",
      "iteration 0 / 1500: loss 1371.909767\n",
      "iteration 100 / 1500: loss 2.938195\n",
      "iteration 200 / 1500: loss 2.116817\n",
      "iteration 300 / 1500: loss 2.149648\n",
      "iteration 400 / 1500: loss 2.145861\n",
      "iteration 500 / 1500: loss 2.145988\n",
      "iteration 600 / 1500: loss 2.129878\n",
      "iteration 700 / 1500: loss 2.126136\n",
      "iteration 800 / 1500: loss 2.109016\n",
      "iteration 900 / 1500: loss 2.157934\n",
      "iteration 1000 / 1500: loss 2.108962\n",
      "iteration 1100 / 1500: loss 2.078821\n",
      "iteration 1200 / 1500: loss 2.131013\n",
      "iteration 1300 / 1500: loss 2.139644\n",
      "iteration 1400 / 1500: loss 2.129697\n",
      "iteration 0 / 1500: loss 1464.691756\n",
      "iteration 100 / 1500: loss 2.668721\n",
      "iteration 200 / 1500: loss 2.113201\n",
      "iteration 300 / 1500: loss 2.157250\n",
      "iteration 400 / 1500: loss 2.154319\n",
      "iteration 500 / 1500: loss 2.114968\n",
      "iteration 600 / 1500: loss 2.174279\n",
      "iteration 700 / 1500: loss 2.141781\n",
      "iteration 800 / 1500: loss 2.176296\n",
      "iteration 900 / 1500: loss 2.124456\n",
      "iteration 1000 / 1500: loss 2.134809\n",
      "iteration 1100 / 1500: loss 2.163397\n",
      "iteration 1200 / 1500: loss 2.168182\n",
      "iteration 1300 / 1500: loss 2.152623\n",
      "iteration 1400 / 1500: loss 2.081109\n",
      "iteration 0 / 1500: loss 1550.733022\n",
      "iteration 100 / 1500: loss 2.485225\n",
      "iteration 200 / 1500: loss 2.094723\n",
      "iteration 300 / 1500: loss 2.159503\n",
      "iteration 400 / 1500: loss 2.110595\n",
      "iteration 500 / 1500: loss 2.131945\n",
      "iteration 600 / 1500: loss 2.135381\n",
      "iteration 700 / 1500: loss 2.160354\n",
      "iteration 800 / 1500: loss 2.126685\n",
      "iteration 900 / 1500: loss 2.103440\n",
      "iteration 1000 / 1500: loss 2.179926\n",
      "iteration 1100 / 1500: loss 2.175735\n",
      "iteration 1200 / 1500: loss 2.151321\n",
      "iteration 1300 / 1500: loss 2.108951\n",
      "iteration 1400 / 1500: loss 2.125908\n",
      "iteration 0 / 1500: loss 766.308235\n",
      "iteration 100 / 1500: loss 9.601090\n",
      "iteration 200 / 1500: loss 2.217711\n",
      "iteration 300 / 1500: loss 2.115097\n",
      "iteration 400 / 1500: loss 2.117744\n",
      "iteration 500 / 1500: loss 2.091770\n",
      "iteration 600 / 1500: loss 2.095226\n",
      "iteration 700 / 1500: loss 2.072761\n",
      "iteration 800 / 1500: loss 2.081547\n",
      "iteration 900 / 1500: loss 2.041018\n",
      "iteration 1000 / 1500: loss 2.067641\n",
      "iteration 1100 / 1500: loss 2.069650\n",
      "iteration 1200 / 1500: loss 2.138308\n",
      "iteration 1300 / 1500: loss 2.041238\n",
      "iteration 1400 / 1500: loss 2.113949\n",
      "iteration 0 / 1500: loss 860.385645\n",
      "iteration 100 / 1500: loss 7.091093\n",
      "iteration 200 / 1500: loss 2.142771\n",
      "iteration 300 / 1500: loss 2.130604\n",
      "iteration 400 / 1500: loss 2.036163\n",
      "iteration 500 / 1500: loss 2.161492\n",
      "iteration 600 / 1500: loss 2.118535\n",
      "iteration 700 / 1500: loss 2.120808\n",
      "iteration 800 / 1500: loss 2.145499\n",
      "iteration 900 / 1500: loss 2.129430\n",
      "iteration 1000 / 1500: loss 2.082185\n",
      "iteration 1100 / 1500: loss 2.028149\n",
      "iteration 1200 / 1500: loss 2.125899\n",
      "iteration 1300 / 1500: loss 2.044808\n",
      "iteration 1400 / 1500: loss 2.113018\n",
      "iteration 0 / 1500: loss 936.795441\n",
      "iteration 100 / 1500: loss 5.400339\n",
      "iteration 200 / 1500: loss 2.089505\n",
      "iteration 300 / 1500: loss 2.084311\n",
      "iteration 400 / 1500: loss 2.116227\n",
      "iteration 500 / 1500: loss 2.090743\n",
      "iteration 600 / 1500: loss 2.107297\n",
      "iteration 700 / 1500: loss 2.088053\n",
      "iteration 800 / 1500: loss 2.105571\n",
      "iteration 900 / 1500: loss 2.102872\n",
      "iteration 1000 / 1500: loss 2.054315\n",
      "iteration 1100 / 1500: loss 2.134821\n",
      "iteration 1200 / 1500: loss 2.129535\n",
      "iteration 1300 / 1500: loss 2.099581\n",
      "iteration 1400 / 1500: loss 2.139639\n",
      "iteration 0 / 1500: loss 1033.344984\n",
      "iteration 100 / 1500: loss 4.296888\n",
      "iteration 200 / 1500: loss 2.077049\n",
      "iteration 300 / 1500: loss 2.041720\n",
      "iteration 400 / 1500: loss 2.135683\n",
      "iteration 500 / 1500: loss 2.100565\n",
      "iteration 600 / 1500: loss 2.092997\n",
      "iteration 700 / 1500: loss 2.095007\n",
      "iteration 800 / 1500: loss 2.107823\n",
      "iteration 900 / 1500: loss 2.126824\n",
      "iteration 1000 / 1500: loss 2.123748\n",
      "iteration 1100 / 1500: loss 2.106704\n",
      "iteration 1200 / 1500: loss 2.138067\n",
      "iteration 1300 / 1500: loss 2.139638\n",
      "iteration 1400 / 1500: loss 2.091124\n",
      "iteration 0 / 1500: loss 1110.170380\n",
      "iteration 100 / 1500: loss 3.500831\n",
      "iteration 200 / 1500: loss 2.123439\n",
      "iteration 300 / 1500: loss 2.124955\n",
      "iteration 400 / 1500: loss 2.092455\n",
      "iteration 500 / 1500: loss 2.127750\n",
      "iteration 600 / 1500: loss 2.169781\n",
      "iteration 700 / 1500: loss 2.053955\n",
      "iteration 800 / 1500: loss 2.075796\n",
      "iteration 900 / 1500: loss 2.132002\n",
      "iteration 1000 / 1500: loss 2.121750\n",
      "iteration 1100 / 1500: loss 2.097269\n",
      "iteration 1200 / 1500: loss 2.134475\n",
      "iteration 1300 / 1500: loss 2.138875\n",
      "iteration 1400 / 1500: loss 2.118799\n",
      "iteration 0 / 1500: loss 1193.918307\n",
      "iteration 100 / 1500: loss 3.023579\n",
      "iteration 200 / 1500: loss 2.125886\n",
      "iteration 300 / 1500: loss 2.117343\n",
      "iteration 400 / 1500: loss 2.122244\n",
      "iteration 500 / 1500: loss 2.176999\n",
      "iteration 600 / 1500: loss 2.110908\n",
      "iteration 700 / 1500: loss 2.142832\n",
      "iteration 800 / 1500: loss 2.163286\n",
      "iteration 900 / 1500: loss 2.135779\n",
      "iteration 1000 / 1500: loss 2.101290\n",
      "iteration 1100 / 1500: loss 2.136333\n",
      "iteration 1200 / 1500: loss 2.138162\n",
      "iteration 1300 / 1500: loss 2.118402\n",
      "iteration 1400 / 1500: loss 2.153417\n",
      "iteration 0 / 1500: loss 1285.291358\n",
      "iteration 100 / 1500: loss 2.731918\n",
      "iteration 200 / 1500: loss 2.113961\n",
      "iteration 300 / 1500: loss 2.131852\n",
      "iteration 400 / 1500: loss 2.117259\n",
      "iteration 500 / 1500: loss 2.151357\n",
      "iteration 600 / 1500: loss 2.149257\n",
      "iteration 700 / 1500: loss 2.138721\n",
      "iteration 800 / 1500: loss 2.089847\n",
      "iteration 900 / 1500: loss 2.087793\n",
      "iteration 1000 / 1500: loss 2.146692\n",
      "iteration 1100 / 1500: loss 2.128353\n",
      "iteration 1200 / 1500: loss 2.147868\n",
      "iteration 1300 / 1500: loss 2.156664\n",
      "iteration 1400 / 1500: loss 2.128604\n",
      "iteration 0 / 1500: loss 1372.728552\n",
      "iteration 100 / 1500: loss 2.473214\n",
      "iteration 200 / 1500: loss 2.143582\n",
      "iteration 300 / 1500: loss 2.131883\n",
      "iteration 400 / 1500: loss 2.133713\n",
      "iteration 500 / 1500: loss 2.132044\n",
      "iteration 600 / 1500: loss 2.142621\n",
      "iteration 700 / 1500: loss 2.114494\n",
      "iteration 800 / 1500: loss 2.214221\n",
      "iteration 900 / 1500: loss 2.146985\n",
      "iteration 1000 / 1500: loss 2.130224\n",
      "iteration 1100 / 1500: loss 2.116289\n",
      "iteration 1200 / 1500: loss 2.142931\n",
      "iteration 1300 / 1500: loss 2.122935\n",
      "iteration 1400 / 1500: loss 2.132643\n",
      "iteration 0 / 1500: loss 1457.642707\n",
      "iteration 100 / 1500: loss 2.401465\n",
      "iteration 200 / 1500: loss 2.112739\n",
      "iteration 300 / 1500: loss 2.109602\n",
      "iteration 400 / 1500: loss 2.143641\n",
      "iteration 500 / 1500: loss 2.124047\n",
      "iteration 600 / 1500: loss 2.143483\n",
      "iteration 700 / 1500: loss 2.093279\n",
      "iteration 800 / 1500: loss 2.160905\n",
      "iteration 900 / 1500: loss 2.173285\n",
      "iteration 1000 / 1500: loss 2.166505\n",
      "iteration 1100 / 1500: loss 2.165210\n",
      "iteration 1200 / 1500: loss 2.091393\n",
      "iteration 1300 / 1500: loss 2.064549\n",
      "iteration 1400 / 1500: loss 2.093025\n",
      "iteration 0 / 1500: loss 1550.170815\n",
      "iteration 100 / 1500: loss 2.304641\n",
      "iteration 200 / 1500: loss 2.132551\n",
      "iteration 300 / 1500: loss 2.136419\n",
      "iteration 400 / 1500: loss 2.191410\n",
      "iteration 500 / 1500: loss 2.166319\n",
      "iteration 600 / 1500: loss 2.132825\n",
      "iteration 700 / 1500: loss 2.153883\n",
      "iteration 800 / 1500: loss 2.095663\n",
      "iteration 900 / 1500: loss 2.114603\n",
      "iteration 1000 / 1500: loss 2.168951\n",
      "iteration 1100 / 1500: loss 2.167265\n",
      "iteration 1200 / 1500: loss 2.152751\n",
      "iteration 1300 / 1500: loss 2.187819\n",
      "iteration 1400 / 1500: loss 2.193381\n",
      "iteration 0 / 1500: loss 772.572251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 6.942765\n",
      "iteration 200 / 1500: loss 2.144293\n",
      "iteration 300 / 1500: loss 2.086945\n",
      "iteration 400 / 1500: loss 2.063462\n",
      "iteration 500 / 1500: loss 2.079275\n",
      "iteration 600 / 1500: loss 2.082904\n",
      "iteration 700 / 1500: loss 2.122551\n",
      "iteration 800 / 1500: loss 2.111398\n",
      "iteration 900 / 1500: loss 2.131010\n",
      "iteration 1000 / 1500: loss 2.066761\n",
      "iteration 1100 / 1500: loss 2.134614\n",
      "iteration 1200 / 1500: loss 2.017573\n",
      "iteration 1300 / 1500: loss 2.081233\n",
      "iteration 1400 / 1500: loss 2.075479\n",
      "iteration 0 / 1500: loss 866.590729\n",
      "iteration 100 / 1500: loss 5.209333\n",
      "iteration 200 / 1500: loss 2.137896\n",
      "iteration 300 / 1500: loss 2.088468\n",
      "iteration 400 / 1500: loss 2.028111\n",
      "iteration 500 / 1500: loss 2.121249\n",
      "iteration 600 / 1500: loss 2.135340\n",
      "iteration 700 / 1500: loss 2.085109\n",
      "iteration 800 / 1500: loss 2.032876\n",
      "iteration 900 / 1500: loss 2.127923\n",
      "iteration 1000 / 1500: loss 2.158736\n",
      "iteration 1100 / 1500: loss 2.140357\n",
      "iteration 1200 / 1500: loss 2.068395\n",
      "iteration 1300 / 1500: loss 2.089334\n",
      "iteration 1400 / 1500: loss 2.022912\n",
      "iteration 0 / 1500: loss 943.281562\n",
      "iteration 100 / 1500: loss 3.955911\n",
      "iteration 200 / 1500: loss 2.123766\n",
      "iteration 300 / 1500: loss 2.229650\n",
      "iteration 400 / 1500: loss 2.104583\n",
      "iteration 500 / 1500: loss 2.144053\n",
      "iteration 600 / 1500: loss 2.145420\n",
      "iteration 700 / 1500: loss 2.050971\n",
      "iteration 800 / 1500: loss 2.111077\n",
      "iteration 900 / 1500: loss 2.076569\n",
      "iteration 1000 / 1500: loss 2.088462\n",
      "iteration 1100 / 1500: loss 2.129550\n",
      "iteration 1200 / 1500: loss 2.093801\n",
      "iteration 1300 / 1500: loss 2.059436\n",
      "iteration 1400 / 1500: loss 2.149420\n",
      "iteration 0 / 1500: loss 1028.489170\n",
      "iteration 100 / 1500: loss 3.292904\n",
      "iteration 200 / 1500: loss 2.052286\n",
      "iteration 300 / 1500: loss 2.140678\n",
      "iteration 400 / 1500: loss 2.121018\n",
      "iteration 500 / 1500: loss 2.111502\n",
      "iteration 600 / 1500: loss 2.130330\n",
      "iteration 700 / 1500: loss 2.057472\n",
      "iteration 800 / 1500: loss 2.086497\n",
      "iteration 900 / 1500: loss 2.130555\n",
      "iteration 1000 / 1500: loss 2.104983\n",
      "iteration 1100 / 1500: loss 2.139996\n",
      "iteration 1200 / 1500: loss 2.112530\n",
      "iteration 1300 / 1500: loss 2.087719\n",
      "iteration 1400 / 1500: loss 2.153283\n",
      "iteration 0 / 1500: loss 1116.364923\n",
      "iteration 100 / 1500: loss 2.804917\n",
      "iteration 200 / 1500: loss 2.134641\n",
      "iteration 300 / 1500: loss 2.067534\n",
      "iteration 400 / 1500: loss 2.199757\n",
      "iteration 500 / 1500: loss 2.123913\n",
      "iteration 600 / 1500: loss 2.134666\n",
      "iteration 700 / 1500: loss 2.124890\n",
      "iteration 800 / 1500: loss 2.126474\n",
      "iteration 900 / 1500: loss 2.086325\n",
      "iteration 1000 / 1500: loss 2.113689\n",
      "iteration 1100 / 1500: loss 2.120241\n",
      "iteration 1200 / 1500: loss 2.136882\n",
      "iteration 1300 / 1500: loss 2.116340\n",
      "iteration 1400 / 1500: loss 2.064136\n",
      "iteration 0 / 1500: loss 1207.971915\n",
      "iteration 100 / 1500: loss 2.535121\n",
      "iteration 200 / 1500: loss 2.134376\n",
      "iteration 300 / 1500: loss 2.116187\n",
      "iteration 400 / 1500: loss 2.116348\n",
      "iteration 500 / 1500: loss 2.146412\n",
      "iteration 600 / 1500: loss 2.133825\n",
      "iteration 700 / 1500: loss 2.147287\n",
      "iteration 800 / 1500: loss 2.173637\n",
      "iteration 900 / 1500: loss 2.164309\n",
      "iteration 1000 / 1500: loss 2.161329\n",
      "iteration 1100 / 1500: loss 2.117833\n",
      "iteration 1200 / 1500: loss 2.098498\n",
      "iteration 1300 / 1500: loss 2.115321\n",
      "iteration 1400 / 1500: loss 2.103104\n",
      "iteration 0 / 1500: loss 1299.027632\n",
      "iteration 100 / 1500: loss 2.372328\n",
      "iteration 200 / 1500: loss 2.091334\n",
      "iteration 300 / 1500: loss 2.116456\n",
      "iteration 400 / 1500: loss 2.111615\n",
      "iteration 500 / 1500: loss 2.202320\n",
      "iteration 600 / 1500: loss 2.174371\n",
      "iteration 700 / 1500: loss 2.148488\n",
      "iteration 800 / 1500: loss 2.159526\n",
      "iteration 900 / 1500: loss 2.117337\n",
      "iteration 1000 / 1500: loss 2.092703\n",
      "iteration 1100 / 1500: loss 2.180248\n",
      "iteration 1200 / 1500: loss 2.158227\n",
      "iteration 1300 / 1500: loss 2.184682\n",
      "iteration 1400 / 1500: loss 2.090108\n",
      "iteration 0 / 1500: loss 1351.509454\n",
      "iteration 100 / 1500: loss 2.300472\n",
      "iteration 200 / 1500: loss 2.099754\n",
      "iteration 300 / 1500: loss 2.159080\n",
      "iteration 400 / 1500: loss 2.181753\n",
      "iteration 500 / 1500: loss 2.159503\n",
      "iteration 600 / 1500: loss 2.160319\n",
      "iteration 700 / 1500: loss 2.149108\n",
      "iteration 800 / 1500: loss 2.136682\n",
      "iteration 900 / 1500: loss 2.122802\n",
      "iteration 1000 / 1500: loss 2.148443\n",
      "iteration 1100 / 1500: loss 2.138363\n",
      "iteration 1200 / 1500: loss 2.166240\n",
      "iteration 1300 / 1500: loss 2.182912\n",
      "iteration 1400 / 1500: loss 2.172995\n",
      "iteration 0 / 1500: loss 1451.093576\n",
      "iteration 100 / 1500: loss 2.228813\n",
      "iteration 200 / 1500: loss 2.158440\n",
      "iteration 300 / 1500: loss 2.170826\n",
      "iteration 400 / 1500: loss 2.163500\n",
      "iteration 500 / 1500: loss 2.148113\n",
      "iteration 600 / 1500: loss 2.140000\n",
      "iteration 700 / 1500: loss 2.123651\n",
      "iteration 800 / 1500: loss 2.103563\n",
      "iteration 900 / 1500: loss 2.147710\n",
      "iteration 1000 / 1500: loss 2.143067\n",
      "iteration 1100 / 1500: loss 2.203064\n",
      "iteration 1200 / 1500: loss 2.122131\n",
      "iteration 1300 / 1500: loss 2.109913\n",
      "iteration 1400 / 1500: loss 2.185307\n",
      "iteration 0 / 1500: loss 1531.654484\n",
      "iteration 100 / 1500: loss 2.225289\n",
      "iteration 200 / 1500: loss 2.168521\n",
      "iteration 300 / 1500: loss 2.185321\n",
      "iteration 400 / 1500: loss 2.154335\n",
      "iteration 500 / 1500: loss 2.121556\n",
      "iteration 600 / 1500: loss 2.111540\n",
      "iteration 700 / 1500: loss 2.135585\n",
      "iteration 800 / 1500: loss 2.128785\n",
      "iteration 900 / 1500: loss 2.149114\n",
      "iteration 1000 / 1500: loss 2.155630\n",
      "iteration 1100 / 1500: loss 2.119705\n",
      "iteration 1200 / 1500: loss 2.146752\n",
      "iteration 1300 / 1500: loss 2.133594\n",
      "iteration 1400 / 1500: loss 2.203993\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.332000 val accuracy: 0.353000\n",
      "best validation accuracy achieved during cross-validation: 0.353000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in np.linspace(learning_rates[0],learning_rates[1],10):\n",
    "    for reg in np.linspace(regularization_strengths[0],regularization_strengths[1],10):\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train,y_train,learning_rate=lr, reg=reg,num_iters=1500, verbose=True)\n",
    "        train_accuracy = np.mean(y_train == softmax.predict(X_train))\n",
    "        val_accuracy = np.mean(y_val == softmax.predict(X_val))\n",
    "        if val_accuracy>best_val:\n",
    "            best_softmax = softmax\n",
    "            best_val = val_accuracy\n",
    "            results[(lr,reg)] = (train_accuracy,val_accuracy)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.347000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
